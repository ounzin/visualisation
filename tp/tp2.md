# TP 2 ‚Äî Pr√©paration des donn√©es avec Python

## Objectif

Nettoyer le dataset `catnat_dirty.csv` et produire un fichier propre `catnat_clean.csv` pr√™t √† √™tre analys√© dans Tableau.

T√©l√©charger `catnat_dirty.csv` [ici](https://drive.google.com/file/d/1jgsiR9bpAoYQ_v6Do9cBdtzFTPZ8NXNp/view?usp=sharing)

## Probl√®mes √† r√©soudre

Le dataset contient volontairement :

- ~150 doublons
- Valeurs manquantes suppl√©mentaires
- Incoh√©rences de casse (Asia, ASIA, asia...)
- Espaces parasites
- Variantes d'orthographe (USA, US, United States...)
- `Start Year` en format texte avec erreurs ("2020 AD", "Year 2020")
- Outliers aberrants (d√©c√®s n√©gatifs, magnitude √† 999)

---

## Mise en place

```python
import pandas as pd
import numpy as np

df = pd.read_csv("catnat_dirty.csv")
```

---

# Exercice 1 ‚Äî Exploration et diagnostic

> **Rappel**
>
> Avant de modifier, toujours explorer :
>
> - `df.shape` ‚Üí dimensions
> - `df.info()` ‚Üí types et nulls
> - `df.describe()` ‚Üí statistiques
> - `df.isnull().sum()` ‚Üí comptage des nulls
> - `df.duplicated().sum()` ‚Üí comptage des doublons
> - `df['col'].value_counts()` ‚Üí valeurs uniques

## √Ä faire

1. Affichez les dimensions du dataset
2. Affichez les types de donn√©es avec `info()`
3. Comptez les valeurs manquantes par colonne (nombre et pourcentage)
4. Comptez le nombre de doublons
5. Affichez les valeurs uniques de `Region` ‚Äî rep√©rez les incoh√©rences
6. Affichez les statistiques de `Total Deaths` ‚Äî rep√©rez les anomalies

## Questions

- Combien y a-t-il de doublons ?
- Quelles colonnes ont le plus de valeurs manquantes ?
- Combien de variantes diff√©rentes pour "Asia" ?
- Y a-t-il des valeurs n√©gatives dans `Total Deaths` ?

---

# Exercice 2 ‚Äî Suppression des doublons

> **Rappel**
>
> ```python
> # Identifier les doublons
> df[df.duplicated()]
> df[df.duplicated(keep=False)]  # Inclut les originaux
>
> # Supprimer les doublons
> df = df.drop_duplicates()
> df = df.drop_duplicates(subset=['col1', 'col2'])  # Sur certaines colonnes
> ```

## √Ä faire

1. Affichez quelques lignes dupliqu√©es pour v√©rifier
2. Supprimez les doublons exacts
3. V√©rifiez que les doublons ont bien √©t√© supprim√©s

---

# Exercice 3 ‚Äî Correction des types

> **Rappel**
>
> ```python
> # V√©rifier les types
> df.dtypes
>
> # Convertir en num√©rique (erreurs ‚Üí NaN)
> df['col'] = pd.to_numeric(df['col'], errors='coerce')
>
> # Convertir en entier
> df['col'] = df['col'].astype(int)
>
> # Convertir en date
> df['col'] = pd.to_datetime(df['col'], errors='coerce')
> ```

```python
# petit hint...
dtype.name existe !

# AD signifie Anno Domini, elle correspond √† la formule fran√ßais apr√®s JC ...
```

## √Ä faire

1. V√©rifiez le type de `Start Year`
2. Affichez quelques valeurs probl√©matiques (contenant "Year" ou "AD")
3. Nettoyez la colonne : supprimez le texte et convertissez en num√©rique
4. V√©rifiez le r√©sultat

---

# Exercice 4 ‚Äî Traitement des valeurs manquantes

> **Rappel**
>
> | Situation                               | Action                             |
> | --------------------------------------- | ---------------------------------- |
> | Peu de nulls, colonne critique          | Supprimer les lignes               |
> | Beaucoup de nulls, colonne non critique | Supprimer la colonne ou garder NaN |
> | Num√©rique, distribution asym√©trique     | Imputer par m√©diane                |
> | Cat√©goriel                              | Imputer par mode ou "Inconnu"      |
>
> ```python
> df.dropna(subset=['col'])           # Supprimer lignes
> df['col'].fillna(valeur)            # Remplacer
> df['col'].fillna(df['col'].median())  # M√©diane
> ```

## √Ä faire

1. Pour `Region` (peu de nulls) : supprimez les lignes avec null
2. Pour `Event Name` : remplacez les nulls par "Non nomm√©"
3. Pour `Total Deaths` : d√©cidez d'une strat√©gie et appliquez-la
4. Pour `Magnitude` : laissez les nulls (n'a pas de sens pour tous les types)
5. V√©rifiez le r√©sultat

---

# Exercice 5 ‚Äî Traitement des outliers

> **Rappel**
>
> Deux questions :
>
> 1. Est-ce une erreur ? ‚Üí Corriger ou supprimer
> 2. Est-ce une valeur extr√™me r√©elle ? ‚Üí Garder (ou analyser s√©par√©ment)
>
> ```python
> # Valeurs impossibles
> df[df['col'] < 0]
> df[df['col'] > seuil_max]
>
> # Supprimer
> df = df[df['col'] >= 0]
>
> # Capper
> df['col'] = df['col'].clip(lower=0, upper=max_val)
> ```

## √Ä faire

1. Identifiez les valeurs n√©gatives dans `Total Deaths`
2. Identifiez les valeurs aberrantes dans `Magnitude` (> 10)
3. D√©cidez : supprimer ou corriger ?
4. Appliquez le traitement
5. V√©rifiez le r√©sultat

---

# Exercice 6 ‚Äî Nettoyage du texte

> **Rappel**
>
> ```python
> # Casse
> df['col'].str.lower()
> df['col'].str.upper()
> df['col'].str.title()
>
> # Espaces
> df['col'].str.strip()
>
> # Remplacement
> df['col'].replace({'old': 'new'})
>
> # Pipeline complet
> df['col'] = df['col'].str.strip().str.lower()
> ```

## √Ä faire

1. Nettoyez `Region` : strip + title case
2. V√©rifiez avec `value_counts()` ‚Äî combien de cat√©gories maintenant ?
3. Nettoyez `Disaster Type` de la m√™me mani√®re
4. Nettoyez `Country` : strip + title case
5. Corrigez les variantes de pays (USA ‚Üí United States, etc.)

---

# Exercice 7 ‚Äî Cr√©ation de colonnes utiles

> **Rappel**
>
> ```python
> # Nouvelle colonne calcul√©e
> df['new'] = df['col1'] / df['col2']
>
> # Colonne √† partir d'une autre
> df['Decennie'] = (df['Year'] // 10) * 10
>
> # Extraction de date
> df['Annee'] = df['Date'].dt.year
> df['Mois'] = df['Date'].dt.month
> ```

## √Ä faire

1. Cr√©ez une colonne `Decennie` √† partir de `Start Year`
2. Cr√©ez une colonne `Has_Deaths` (bool√©en : True si Total Deaths > 0)
3. V√©rifiez vos nouvelles colonnes

---

# Exercice 8 ‚Äî S√©lection et export

> **Rappel**
>
> ```python
> # S√©lectionner des colonnes
> df_export = df[['col1', 'col2', 'col3']]
>
> # Renommer
> df_export = df_export.rename(columns={'old': 'new'})
>
> # Export
> df_export.to_csv("fichier.csv", index=False)
> ```

## √Ä faire

1. S√©lectionnez les colonnes utiles pour l'analyse
2. Renommez les colonnes si n√©cessaire (noms clairs, sans espaces)
3. Faites une v√©rification finale avec `info()` et `head()`
4. Exportez en CSV : `catnat_clean.csv`

---

# Exercice 9 ‚Äî V√©rification dans Tableau

## √Ä faire

1. Ouvrez Tableau Public
2. Connectez-vous au fichier `catnat_clean.csv`
3. V√©rifiez dans l'√©cran "Source de donn√©es" :
   - Les types sont-ils corrects ? (# pour nombres, Abc pour texte)
   - Les nombres sont-ils bien reconnus ?
4. Cr√©ez un bar chart : `Type_Catastrophe` vs `COUNT(ID_Catastrophe)`
   - Les cat√©gories sont-elles propres ? (pas de doublons)
5. Cr√©ez un bar chart : `Region` vs `SUM(Deces)`
   - Les 5 r√©gions sont-elles bien distinctes ?
6. Cr√©ez une carte avec `Country`
   - Les pays sont-ils reconnus ?

---

## Pour les curieux ü§®

- **Automatiser** : transformez ce notebook en script r√©utilisable
- **Documenter** : ajoutez des commentaires expliquant chaque d√©cision
- **Versionner** : utilisez Git pour suivre les modifications
- **Profiling** : testez `pandas-profiling` pour un diagnostic automatique

```python
# Installation
# pip install pandas-profiling

from pandas_profiling import ProfileReport
profile = ProfileReport(df, title="Rapport de qualit√©")
profile.to_file("rapport_qualite.html")
```
